+- - - - - - - - - - - - - - - - - - hustler@kwork -- - - - - - - - - - - - - - - - - -+

Memory Layout on AArch64 Linux

    +----------------------------------------------------------------------------------+
    | Start               End                      Size      Use                       |
    | ---------------------------------------------------------------------------------+
    | 0000000000000000    0000ffffffffffff         256TB     user                      |
    | ffff000000000000    ffff7fffffffffff         128TB     kernel logical memory map |
    |[ffff600000000000    ffff7fffffffffff]         32TB     [kasan shadow region]     |
    | ffff800000000000    ffff800007ffffff         128MB     modules                   |
    | ffff800008000000    fffffbffefffffff         124TB     vmalloc                   |
    | fffffbfff0000000    fffffbfffdffffff         224MB     fixed mappings (top down) |
    | fffffbfffe000000    fffffbfffe7fffff           8MB     [guard region]            |
    | fffffbfffe800000    fffffbffff7fffff          16MB     PCI I/O space             |
    | fffffbffff800000    fffffbffffffffff           8MB     [guard region]            |
    | fffffc0000000000    fffffdffffffffff           2TB     vmemmap                   |
    | fffffe0000000000    ffffffffffffffff           2TB     [guard region]            |
    +----------------------------------------------------------------------------------+

Translation table lookup with 4KB pages

    +--------+--------+--------+--------+--------+--------+--------+--------+
    |63    56|55    48|47    40|39    32|31    24|23    16|15     8|7      0|
    +--------+--------+--------+--------+--------+--------+--------+--------+
     |                 |         |         |         |         |
     |                 |         |         |         |         v
     |                 |         |         |         |   [11:0 ] in-page offset
     |                 |         |         |         +-> [20:12] L3 index
     |                 |         |         +-----------> [29:21] L2 index
     |                 |         +---------------------> [38:30] L1 index
     |                 +-------------------------------> [47:39] L0 index
     +-------------------------------------------------> [63]    TTBR0/1

ARMv8.5 based processors introduce the Memory Tagging Extension (MTE) feature. MTE is
built on top of the ARMv8.0 virtual address tagging TBI (Top Byte Ignore) feature and
allows software to access a 4-bit allocation tag for each 16-byte granule in the
physical address space.

----------------------------------------------------------------------------------------
- HUGE PAGES -

To avoid spending precious processor cycles on the address translation, CPUs maintain a
cache of such translations called Translation Lookaside Buffer (or TLB).

Huge Pages significantly reduces pressure on TLB, improves TLB hit-rate and thus improves
overall system performance.

1) HugeTLB filesystem (hugetlbfs), a pseudo filesystem that uses RAM as its backing store.
2) Transparent HugePages (THP)

----------------------------------------------------------------------------------------
- VIRTUALLAY CONTIGUOUS MEMORY -

Tips:

@include/linux/mm.h

0) page_to_virt()
1) vmalloc_to_page()
2) vmalloc_to_pfn()

struct vm_struct - kernel vmalloc area descriptor

struct vmap_block -> va
                      |
             struct vmap_area
             [va_start:va_end]

vmap_area_list
purge_vmap_area_list
free_vmap_area_list

vmap_area_root - 'busy' vmap area rb-tree root
purge_vmap_area_root - purge vmap area rb-tree root
free_vmap_area_root - 'free' vmap area rb-tree root 

@mm/vmalloc.c

vmalloc() - Allocate virtually contiguous memory with given size
   |
   |          +<<- [3]
   |          |
   +- __vmalloc_node()
              |
              +- __vmalloc_node_range()
                            |
                            +- __get_vm_area_node()
                           [2]          |
                                        |- ...
                                        | 
                                        +- alloc_vmap_area()
                                       [1]         |
                                           Allocate a region of KVA of the
                                           specified size and alignment,
                                           within [vstart, vend]
                                                   |
                        free_vmap_area_root/list ->+- __alloc_vmap_area()
                                  |                          |
                                  +------------------------>>+- find_vmap_lowest_match()
                                                             |             |
                                                            [5]            |
                                                                Find the first free block
                                                                (lowest start address) in
                                                                the rb-tree.

                                                            [5]
                                                             |
                                                             +- adjust_va_to_fit_type()
                                                                         |
                                                              Update the free vmap_area

                                       [1]
                                        |                   
                  vmap_area_root/list ->+- insert_vmap_area()
                          |                         |
                          |                         |- find_va_links()
                          |                         |
                          +----------------------->>+- link_va()

  [2]
   |
   +- __vmalloc_area_node() - Allocate physical pages and map them into vmalloc space.                        
                |
                +- (array_size > PAGE_SIZE) ->> [3]
               [7]           |no 
                             +- kmalloc_node() @include/linux/slab.h
                                      |
                                      +- __kmalloc_node()
                                                 |
                                                 +- __do_malloc_node()
                                                    @mm/slab_common.c
               [7]
                |
                +- vm_area_alloc_pages()
               [4]          |
                            +- When order=0, use bulk allocator
                           [6] 1) alloc_pages_bulk_array_mempolicy() @mm/mempolicy.c
                                                |
                                                +- consider mempolicy
                                                           |
                                                           +- __alloc_pages_bulk()
                                                              @mm/page_alloc.c
                                                                     |
                               2) alloc_pages_bulk_array_node()      |
                                               |                     |
                                               +- consider node id   |
                                                          |          | 
                                                          +----------+
                           [6]
                            |
                            +- When order>0, or bulk allocator fails.
                               1) alloc_pages()
                               2) alloc_pages_node()
                               then split_page()
                                         |
                                         +- Takes a non-compound higher-order page, 
                                            and splits it into n (1 << order) sub-
                                            pages: page[0..n].

               [4]
                |
                +- map pages to a kernel virtual address
                   vmap_pages_range() 
                           |
                           +- vmap_pages_range_noflush()
                                         |
                                         +- __vmap_pages_range_noflush()
                                                        |
                                                        +- vmap_range_noflush()
                                                                   |
                                                      To build page table [layout]
                                                                   |
            +-------------+-------------+------------+-------------+-------------+ 
            |     PGD     |     P4D     |     PUD    |     PMD     |     PTE     |
            +-------------+-------------+------------+-------------+-------------+
                   |             |             |            |             |
                   |             |             |            |             |
                   +- - - - - - >|             |            |             |
             vmap_p4d_range      |             |            |             |
                                 +- - - - - - >|            |             |
                           vmap_pud_range      |            |             |
                                               +- - - - - ->|             |
                                         vmap_pmd_range     |             |
                                                            +- - - - - - >|
                                                      vmap_pte_range      |            
                                                                        [end]

VMALLOC_SPACE

+ - - - - + - VMALLOC_START
|         |
|         |
.         .
.         .

|         |
|         |
+ - - - - + - VMALLOC_END

----------------------------------------------------------------------------------------
- ADDRESS TRANSLATION -

a) Address Space Identifiers (ASIDs)
            |
            +- Tagging translations with the owning process

TLB entries for multiple processes are allowed to coexist in the cache, and the ASID
determines which entry to use.

b) Virtual Machine Identifiers (VMIDs)
            |
            +- Tagging translations with the owning VM

VMIDs allow translations from different VMs to coexist in the cache.

MMU - Memory Management Unit

* The table walk unit - contains logic that reads the translation tables from memory.
* Translation Lookaside Buffers (TLBs) - cache recently used translations.

Translation Table Base Registers

ttbr0_el1
ttbr1_el1
   |
   +- Holds the base address of the translation table for the initial lookup for stage 1
      of the translation of an address from the higher VA range in the EL1&0 stage 1
      translation regime, and other information for this translation regime.

----------------------------------------------------------------------------------------
- BOOTTIME MEMORY MANAGEMENT -

start_kernel() @init/main.c
      |
      +- page_address_init() @mm/highmem.c
      |
      +- setup_arch() @arch/arm64/kernel/setup.c
              |
              +- paging_init() @arch/arm64/mm/mmu.c
              |       |
             [0]      +- map_kernel
                      |
                      +- map_mem

             [0]
              |
              +- bootmem_init() @arch/arm64/mm/init.c
                      |
                      +- early_memtest()
                      |
                      +- arch_numa_init()
                      |
                      +- arm64_hugetlb_cma_reserve()
                      |  Reserve CMA areas for the largest supported gigantic huge page
                      |  when requested.
                      |
                      +- dma_pernuma_cma_reserve()
                      |
                      +- kvm_hyp_reserve()
                      |
                      +- sparse_init()
                      |
                      +- zone_sizes_init()
                      |         |
                     [1]        +- free_area_init() @mm/page_alloc.c
                                          |
                                          +- free_area_init_node()
                                                [To each node]
                                                      |
                                                      +- free_area_init_core()
                                                             |
                                                             +- pgdat_init_internals()
                                                             |
                                                             +- zone_init_internals()
                                                                   [To each zone]
                     [1]              
                      |
                      +- dma_contiguous_reserve()

----------------------------------------------------------------------------------------
- BUDDY SYSTEM -

(1) CONFIG_NUMA=y

alloc_pages() 
     |
     +- 1) pol->mode == MPOL_INTERLEAVE
     |                |
     |     alloc_page_interleave()
     |
     +- 2) pol->mode == MPOL_PREFERRED_MANY
     |                |
     |     alloc_pages_preferred_many()
     |
     +- 3) __alloc_pages()

(2) CONFIG_NUMA=n

alloc_pages()
     |
     +- alloc_pages_node()
                |
                +- __alloc_pages_node()
                             |
                             +- __alloc_pages()

$ cat /proc/pagetypeinfo

struct free_area

+------------+                        +---------------------+
| order = 0  | -> struct free_list -> | MIGRATE_UNMOVABLE   |
+------------+                        +---------------------+
|            |                        | MIGRATE_MOVABLE     |
:            :                        +---------------------+
|            |                        | MIGRATE_RECLAIMABLE |
+------------+                        +---------------------+
| order = 11 |                        | MIGRATE_HIGHATOMIC  |
+------------+                        +---------------------+
                                      | MIGRATE_ISOLATE     |
                                      +---------------------+

__alloc_pages() with struct alloc_context ac instantiated
       |
       +- prepare_alloc_pages()
       |           |
      [2]          +- Initialize the alloc_context struture

      [2]    
       |
       +- get_page_from_freelist()
       |           |
      [3]          +- 1) scan zonelist, looking for a zone with enough free
                         a. Check the watermark via zone_watermark_fast()
                            if false, try to reclaim via node_reclaim() when needed;
                            if true, try to allocate from current zone;
                         b. Allocate pages from given zone via rmqueue()
                               |
                           [x]-+- When order=0, allocate via rmqueue_pcplist()
                                          |
                                          +-

                           [x]-+- When order>0, allocate via rmqueue_buddy()
                                          |
                                          +- Try __rmqueue_smallest() first
                                          |           |
                                         [4]          +- Go through the free lists for
                                                         the given migratetype and remove
                                                         the smallest available page from
                                                         the freelist
                                                         a) get_page_from_free_area()
                                                         b) del_page_from_free_list()
                                                         c) if current order doesn't meet,
                                                         go to higher order, if available,
                                                         split them into half, put a half
                                                         back to lower order of free area,
                                                         another half as requested.
                                                         - expand()
                                         [4]
                                          |
                                          +- if fails, do __rmqueue()
                                                               |
                                                               +- CONFIG_CMA=y
                                                                  __rmqueue_cma_fallback()
                                                               
                                                               |
                                                               +- Try __rmqueue_smallest()
                                                                  again. If fails, then do
                                                                  1) __rmqueue_cma_fallback()
                                                                  2) __rmqueue_fallback()
                                                                             |
                             Try finding a free buddy page on the fallback list and put it
                             on the free list of requested migratetype.
                             a) find_suitable_fallback() to find largest/smallest available
                                free page in other list.
                             Note: fallback list can be (one for current, other two fallback)
                             MIGRATE_UNMOVABLE, MIGRATE_MOVABLE, MIGRATE_RECLAIMABLE
                             b) get_page_from_free_area()
                             c) steal_suitable_fallback()
                                        |
                                        +- when whole_block=true, do move_freepages_block()
                                                                                 |
                                       Move the free pages in a range to the freelist tail
                                       of the requested type via move_freepages()
                                                                     |
                                                      loop to move_to_free_list()

                           [x]-+- Test on zone->flags - ZONE_BOOSTED_WATERMARK
                                                                   |
                                                                   +- wakeup_kswapd()


      [3]
       |
       +- __alloc_pages_slowpath()
                   |
                   +- try get_page_from_freelist() <--------------+
                   |  Since alloc_flags changed                   |
                   |                                             [5]
                   +- __alloc_pages_direct_compact()
                   |  If costly order
                   |
             +---->+- alloc_flags & ALLOC_KSWAPD
             |     |  Then wake_all_kswapds()
            [6]    |              |
                   |              +- wakeup_kswapd()
                   |                 on each zone
                   |
                   +- try get_page_from_freelist()
                   |  Since adjusted zonelist and alloc_flags
                   |
                   +- __alloc_pages_direct_reclaim()
                   |               |
                  [7]              +- __perform_reclaim()
                                   |           |
                                               +- try_to_free_pages() @mm/vmscan.c
                                   |
                                   +- get_page_from_freelist()
                                   |
                                   +- drain_all_pages()
                                      When above fails, try to spill all the per-cpu
                                      pages from all CPUs back into the buddy allocator
                                             |
                                             +- __drain_all_pages()
                                                Optimized to only execute on CPUs where
                                                pcplists are not empty, with these CPUs
                                                |
                                                +- drain_pages_zone()
                                                           |
                                                           +- free_pcppages_bulk()

                  [7]
                   |
                   +- __alloc_pages_direct_compact()
                   |                |
                  [8]               +- try_to_compact_pages() @mm/compaction.c
                                    +- get_page_from_freelist()
                                    +- compaction_defer_reset()
                                       Update defer tracking counters after successful
                                       compaction of given order.
            [6]   [8]
             |     |
             +<----+- should_reclaim_retry()
             |     |                                               
             +<----+- should_compact_retry()
                   |                                                
                   +- check_retry_cpuset()                       [5]
                      OR                                          |
                      check_retry_zonelist() -------------------->+
 

                   |
                   +- __alloc_pages_may_oom()

                   |
                   +- When hit nopage
                      __alloc_pages_cpuset_fallback()
                      
----------------------------------------------------------------------------------------
- Direct Reclaim -

try_to_free_pages() with struct scan_control sc initialized
        |
        +- do_try_to_free_pages() as the main entry point to direct page reclaim
                     |
                     +- sc->proactive --------+
                             |yes             |no
                        shrink_zones()        +- vmpressure_prio()
                             |
                             +- shrink_node()
                                      |
                                      +- prepare_scan_count()
                                      |
                                      +- shrink_node_memcgs()
                                                  |
                                                  +- mem_cgroup_iter() iterate the memcgs
                                                  |                                  |
                                                  +- mem_cgroup_lruvec()             |
                                                  |                                  |
                                                  +- shrink_lruvec()                 |
                                                  |                                  |
                                                  +- shrink_slab()                   |
                                                  |                                  |
                                                  +- sc->proactive                   |
                                                         |no                         |
                                                     vmpressure() ------------------>+

@include/linux/mmzone.h

typedef struct pglist_data {
    ...
    struct lruvec __lruvec; ---------+
    ...                              |
} pg_data_t;                         |
                                     \
                                       struct lruvec {
                                            struct list_head lists[NR_LRU_LISTS];
                                            ...                         |
                                       };                               |
                                                                        |
                                                                        /
                                                            +-------------------+
                                                            | LRU_INACTIVE_ANON |
                                                            +-------------------+
                                                            | LRU_ACTIVE_ANON   |
                                                            +-------------------+
                                                            | LRU_INACTIVE_FILE |
                                                            +-------------------+
                                                            | LRU_ACTIVE_FILE   |
                                                            +-------------------+
                                                            | LRU_UNEVICTABLE   |
                                                            +-------------------+

mem_cgroup_lruvec() to get the LRU list vector for a memcg & node
        |
        +- a) (struct pglist_data) pgdat->__lruvec
           b) (struct mem_cgroup_per_node) mz->lruvec

shrink_lruvec()
      |
      +- shrink_list()
               |
               +- is_active_lru() ---------------------------------------------------+
                       |no                                                           |
                       +- shrink_inactive_list()                  shrink_active_list()
                             |
                             +- Isolate page from the lruvec
                             |  to fill in @dst list by
                                nr_to_scan times via
                                isolate_lru_folios()
                                         |
                                         +-

                             |
                             +- shrink_folio_list()
                             |           |
                                         +-

                             |
                             +- Move folios from private @list
                             |  to appropriate LRU list via
                                move_folios_to_lru()
                                         |
                                         +-
shrink_slab()
      |
      +- shrink_slab_memcg()
      |

      |
      +- go through shrinker in shrinker list
         do_shrink_slab()

----------------------------------------------------------------------------------------
- Control Group -

cgroup is a mechanism to organize proccesses hierarchically and distribute system resources
along the hierarchy in a controlled and configurable manner.

cgroups form a tree structure and every process in the system belongs to one and only one
cgroup.


----------------------------------------------------------------------------------------
- Page Swap -

wakeup_kswapd()
      |
      +- Check if pgdat->kswapd_wait is active ---> [END]
                         |
                         |yes
                         |
                         +- a) When kswapd fails over MAX_RECLAIM_RETRIES  ---->+
                            b) When have enough free memory available, but      |
                               too fragmented for high-order allocations.  ---->+
                                |                                               |
                                +- pgdat_balanced()                             |
                                                                                |
                                                            Not __GFP_DIRECT_RECLAIM
                                                                                |
                                                                                v
                                                                  wakeup_kcompactd()

                         |
                         +- wake_up_interruptible(&pgdat->kswapd_wait)
                                                               |
             +<------------------------------------------------+
             |
The Background Pageout Daemon
             |
             +- kswapd()
                   |
                   +- [Infinite Loop]
                             |
                             +- kswapd_try_to_sleep()
                             |
                             +- For kswapd, it will reclaim pages across a node
                                from zones that are eligible for use by the caller
                                until at least one zone is balanced.
                                      |
                                      v
                                balance_pgdat() with struct scan_control sc initialized
                                      |
                    +---------------->+
                    |                 |
                    |                 +- pgdat_balanced()
                    |                 |
                    |                 +- kswapd_shrink_node()
                    |  sc.priority>=1 |
                    +<----------------+

$ sar -B 1
----------------------------------------------------------------------------------------
- MEMORY COMPACTION -

As the system runs, tasks allocate and free the memory and it becomes fragmented. memory
compaction addresses the fragmentation issues. This mechanism moves occupied pages from
the lower part of a memory to free pages in the upper part of the zone.

Before Compaction:

+---------------------+
|x| | |x|x| | | |x|x| |
+---------------------+

After Compaction:

+---------------------+
| | | | | | |x|x|x|x|x|
+---------------------+

try_to_compact_pages() for high-order allocation
        |
        +- compact each zone in the zonelist
                      |
                      +- compact_zone_order() with struct compact_control cc initialized
                                      |
                                      +- compact_zone()
                                               |
                                               +- initialize two lists:
                                                  a) cc->freepages
                                                  b) cc->migratepages
                                               +- compaction_suitable() check if should
                                                  do compaction
                                                           |
                                                           +- __compaction_suitable()
                                                           |    |
                                                          [0]   +- Check if watermarks
                                                                   for high-order
                                                                   allocation are met
                                                                            |
                                                                   zone_watermark_ok()
                                                                            |
                                                                   Possible outcomes
                                                                   a) COMPACT_CONTINUE
                                                                   b) COMPACT_SKIPPED
                                                                   c) COMPACT_SUCCESS
                                                          [0]
                                                           |
                                                           +- fragmentation_index()
                                                              | @mm/vmstat.c
                                                              +- __fragmentation_index()
                                                                           |
        +<----------------------------------------------------+- Possible outcomes
        |                                                        a) 0 => lack of memory
        |                                                        b) 1 => fragmentation
        |
        +- setup for where to start the scanners
        |
        +- loop to do compaction
                          |
                          +- isolate_migratepages()
                          |           |
                         [1]          +- Briefly search the free lists for a migration
                                         source that already has some free pages to
                                         reduce the number of pages that need migration
                                         before a pageblock is free.
                                                   |
                                         fast_find_migrateblock()

                                      |
                         [1]<---------+- a) ISOLATE_ABORT
                          |
                          +- putback_movable_pages() [cc->migratepages = 0]

                                         b) ISOLATE_NONE
                                         c) ISOLATE_SUCCESS
                          |
                          +- migrate_pages() @mm/migrate.c

----------------------------------------------------------------------------------------
- Page Faults -

@include/linux/sched.h

struct task_struct {
    struct thread_info thread_info;
    ...
    void *stack;
    ...
    struct mm_struct *mm;
    struct mm_struct *active_mm;
    ...
    pid_t pid;
    ...
    struct signal_struct *signal;
    struct signal_struct __rcu *sighand;
    ...
    struct thread_struct thread; - CPU-specific state of the task
};
                 |
                 | @arch/arm64/include/asm/processor.h
                 \
                   struct thread_struct {
                        struct cpu_context cpu_context; - CPU registers
                        ...
                        unsigned long fault_address;
                        unsigned long fault_code;
                        struct debug_info debug;
                        ...
                   };


@include/linux/mm.h

vm_fault is filled by the pagefault handler and passed to the vma's fault callback function.

struct vm_fault {
    const struct {
        struct vm_area_struct *vma;
        ...
        unsigned long address;
        unsigned long real_address;
    };
    ...
    struct page *cow_page;
    struct page *page;
    ...
};

A VM area is any part of the process virtual memory space that has a special rule for page-
fault handlers.

@include/linux/mm_types.h

struct vm_area_struct {
    ...
    const struct vm_operations_struct *vm_ops;
    ...                 |
                        +----> ...
                        |
};                      +----> vm_fault_t (*fault)(struct vm_fault *vmf);
                        |
                        +----> vm_fault_t (*huge_fault)(struct vm_fault *vmf,
                                               enum page_entry_size pe_size);


@arch/arm64/kernel/entry.S

Exceptions are conditions or system events that require some action by privileged software
(an exception handler) to ensure smooth functioning of the system.

       Program Flow +
                    |       +
                    |     / |
                    |    /  |
                    |   /   |
                    |  /    |
                    | /     v
   Exception occurs +       + ASM Exception Handler -> C Subroutine
                    | \     |
                    |  \    |
                    |   \   |
                    |    \  |
                    |     \ v spsr_el[n]
                    v       + elr_el[n] - Exception Link Register
                                                     |
                                          When taking an exception to EL1, holds the
                                          address to return to.

Causes to Exception:
1) Aborts - a) Failed instruction fetches (Instruction Aborts)
     |      b) Failed data accesses (Data Aborts)
     |                  |
     |                  +- Error response on a memory access (indicating perhaps that the
     |                     specified address does not correspond to real memory in the
     |                      system)
     |
     +- far_el[n] - Holds the faulting virtual address for all synchronous instruction
                    abort exceptions, data abort exceptions, PC alignment fault
                    exceptions and watchpoint exceptions that are taken to EL1.

The virtual address of each table base is set by the Vector Base Address Registers:
a) vbar_el3
b) vbar_el2
c) vbar_el1

If an exception is taken, the PSTATE information is saved in the Saved Program Status
Registers (spsr_el3, spsr_el2, spsr_el1).

|31                                                         0|
+-+-+-+-+---------+--+--+-----------------+-+-+-+-+-+-+------+
|N|Z|C|V|         |SS|IL|                 |D|A|I|F| |M|M[3:0]|
+-+-+-+-+---------+--+--+-----------------+-+-+-+-+-+-+------+
                                           | | | |
                                           | | | +- FIQ interrupt process state mask
                                           | | +--- IRQ interrupt process state mask
                                           | +----- SError interrupt process state mask
                                           +------- Debug exception mask

exception vectors
   |
vectors
   |
   +- ...
   |
   +- kernel_ventry 0, t, 64, sync --->+
   |                                   |
   +- ...                              |
   |                                   |
   +- kernel_ventry 1, h, 64, sync     |
   |                                   |
   +- ...                              +- entry_handler 0, t, 64, sync
                                       |    (Early Exception Handler)
@arch/arm64/kernel/entry-common.c      |
                                       |
   +------------------------+          |
   |                        |          v
   | el0t_64_sync_handler() <----------+
   |    |                   |
   | el0_da() ------------------------>+
   |    |                   |          |
   | el0_ia() ------------------------>+
U  |                        |          |
---+------------------------+          |
K  |                        |          |
   | el1h_64_sync_handler() |          |
   |     |                  |          |
   | el1_abort() --------------------->+
   |                        |          |
   +------------------------+          |
                                       |
@arch/arm64/mm/fault.c                 |
                                       v
do_mem_abort() <-----------------------+
      |
      +- (struct fault_info) inf->fn() callback
                                   |
                                   /
                                  /
                                 /
                                /
                               /

static const struct fault_info fault_info[] = {
    ...
    { do_translation_fault, SIGSEGV, SEGV_MAPERR, "level 0 translation fault" },
    ...                                               [0, 3]
    { do_page_fault, SIGSEGV, SEGV_ACCERR, "level 1 access flag fault" },
    ...                                        [1, 3]
    { do_page_fault, SIGSEGV, SEGV_ACCERR, "level 1 permission fault" },
    ...                                        [1, 3]
};         |
           |                                        \
           v                                        |
do_translation_fault() @arch/arm64/mm/fault.c       |
       |                                            |
       +- do_page_fault()                           |
                |                                   |
                +- __do_page_fault()                |
                |          |                        |
                |         [0]                       |
                |                                   |
                +- esr_to_fault_info() ------------>+
                |
                +- set_thread_esr()

                          [0]
                           |
                           +- handle_mm_fault()
                                      |
                                      +- is_vm_hugetlb_page() ------->+
                                                  |                   |
                                           hugetlb_fault()            |
                                                             __handle_mm_fault()
                                                             |
                                                             :
                                                             |
                                                             +- handle_pte_fault()
                               vmf->pte                         |       |
    +<----------------------------------------------------------+       | no vmf->pte
    |                                                                   |
do_fault() @mm/memory.c                                         vma_is_anonymous()
    |                                                           |yes        |
    +- When vma->vm_ops->fault                       do_anonymous_page()    |no
                  |                                                         |
       vmf->flags & FAULT_FLAG_WRITE --+                             pte_present()
                  |no                  |                             |no        |
            do_read_fault()            |yes                   do_swap_page()    |yes
                  |                    |                                        |
                 [a]        vma->vm_flags & VM_SHARED --+               pte_protnone()
                                       |no              |               |yes        |
                                 do_cow_fault()         |yes      do_numa_page()    |no
                                       |                |                           |
                                      [b]     do_shared_fault()                     :
                                                        |                           |
                                                       [c]          update_mmu_cache()

Above all follows the logic below:

__do_fault()
     |
     +- When no vmf->prealloc_pte
     |            |
     |            +- pte_alloc_one(vma->vm_mm)
     |
     +- vma->vm_ops->fault() callback

----------------------------------------------------------------------------------------
- PAGE MIGRATION -

Migrate the pages specified in a list, to the free pages supplied as the target for the
page migration.

migrate_pages() @mm/migrate.c
      |
      +- 10 attempts or if no pages are movable any more
                              |
                              +- PageHuge() ---------------> unmap_and_move_huge_page()
                                     |
                                     +- unmap_and_move()
                                               |
                                               +- Obtain the lock on page, remove all
                                                  ptes and migrate the page to the
                                                  newly allocated page in newpage.
                                                                 |
                                                                 +- __unmap_and_move()


----------------------------------------------------------------------------------------
- SLAB -








+- - - - - - - - - - - - - - - - - - hustler@kwork -- - - - - - - - - - - - - - - - - -+
